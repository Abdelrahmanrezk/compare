{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data\n",
    "\n",
    "<img src=\"images/scrap_pipeline.png\">\n",
    "\n",
    "For each year we get the months of that year, then find the days of that month in that specific year, next we move to handle months from 1 to 9 to be 01 to 09, required for start time of scraping, then we handle the direction of that month in that year to save the scrapped tweets inside csv file in that direction, not just for days, we have go deeper in hours of days and scrap data based on some hours of the day, and this help us scraped tweets along the day.\n",
    "\n",
    "\n",
    "### Scrapped tweets more than: 377\n",
    "\n",
    "\n",
    "\n",
    "## Work in our private repo github\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Crawling-twitter-Data-using-twint_v2/blob/main/twitter_configs.py\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Crawling-twitter-Data-using-twint_v2/blob/main/Twitter%20Crawling%202016%20to%202022.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing\n",
    "\n",
    "<img src=\"images/Text-preprocess_pipeline.png\">\n",
    "\n",
    "Arabic language is special in its pre-process steps, beside of that the tweets has some of tokens need to handle.\n",
    "\n",
    "First of all as we get tweet text we get along with its language written in, so first step was to get just the Arabic tweets because some other language like [Urdu and Persian] shared common and have similarity with Arabic language.\n",
    "\n",
    "Next we have moved to tweets itself, and for each tweet we also detect its language from its text, and this is help us a lot to save only Arabic tweets.\n",
    "\n",
    "Once we detect that tweet is Arabic, we need to clean it from some special characters, handle more spaces and all of the required preprocessing as in graph.\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Crawling-twitter-Data-using-twint_v2/blob/main/ara_vec_preprocess_configs.py\n",
    "\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Crawling-twitter-Data-using-twint_v2/blob/main/Text-Preprocessing.ipynb\n",
    "\n",
    "## How number of tweets changes during the pipeline\n",
    "\n",
    "**from about 377 Million tweet to about 267 Million based on Arabic language column.**\n",
    "\n",
    "**from about 267 Million tweet to about 240 Million After remove duplicates.**\n",
    "\n",
    "\n",
    "**from about 240 Million tweet to about 185 Million After detect Arabic tweet.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization on more than 185 Million\n",
    "Lastly we move to the **tokenization**.\n",
    "The pre-process we have done before help us in this stage, when we compared different tokenizer in the first days of work, and we use these tokenizer to split our tweets.\n",
    "\n",
    "## Work in our private repo github\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Crawling-twitter-Data-using-twint_v2/blob/main/DifferentTokenization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Word Vector\n",
    "\n",
    "<img src=\"images/word_2vec_pipeline.png\">\n",
    "\n",
    "Previously we have deal with Bag of words either binary, count or tf-idf, all of these bag of words was ignoring the context of words, the relationship between near words and its effect on the meaning. \n",
    "\n",
    "The BOW was collect all the words into statistical bag of words, then each of these vectorization method assign just one number to the words, like the number of times word appear in sentence or just 1 or 0 if its comes in sentence or not.\n",
    "\n",
    "But is there is a similarity between words like **hotel and motel** ? in the BOW we dealing with there is zero similarity although they represent the same meaning and comes in similar context.\n",
    "\n",
    "**But Why BOW work and give us a pretty good accuracy ?**\n",
    "\n",
    "the BoW with text features model we built (bin, freq, cnt, tfidf), builds a vector per sentence, not word, so tweets(sequence of words in text) with similar words will have 1â€™s at the same location, so the similarity was between two tweets, document (some sequence of words), **not between words itself**, But ? what about (sparsity, missing sequence, information and context of words) ?\n",
    "\n",
    "<img src=\"images/BOW_vec1.png\">\n",
    "\n",
    "**Can you imagine this vector with all of words in your data, and all of these words are 0 because its not appear in the document you process for now and just some numbers for words comes in that document !!**\n",
    "\n",
    "## BUT Word Vector came to light\n",
    "\n",
    "Instead of this large statistical Bag of words **to represent each sentence** (with all words of data), we just compress to some fixed length may be just **50 or 100 words !** (solve the sparsity problem), and we can do that by giving each word **index** so when we need any word we get via its index **Later will know why**.\n",
    "\n",
    "<img src=\"images/word_vec2.png\">\n",
    "\n",
    "**Why using index while its as we can see has no any information or meaning about the word !?**\n",
    "\n",
    "We do not use the index its self to represent the word, we use it to get the word from the Embedding matrix we learn about the words in, and as we can see in the other side of the image the words comes from some topic that represent the word in some space, **But We can feed the words to the model** so we feed **one hot vector of all zero except the index of that word in the matrix **.\n",
    "\n",
    "\n",
    "so We want a mapping from discrete (categorical) space to Vector space, space where the value representing similar meanings are near each others! **This mapping what we called Embedding**\n",
    "\n",
    "**Word Vector are numerical representation of word semantics or meaning including literal and implied meaning. So word vector can capture the connotation of words like [Peopleness, animalness and even conceptness], and they combine all that into a dense vector(no zeros as in sparse vector with BOW) this dense vector is of floating point values, which enables us for queries and logical reasoning.**\n",
    "\n",
    "<img src=\"images/word_vec3.png\">\n",
    "- These **e** are the latten factors represent the word\n",
    "- Also its the features of the word in some space But !!\n",
    "\n",
    "**But what these Embedding represent ? how much embedding we need ?**\n",
    "\n",
    "Lets go back for few seconds in world of machine learning, where these embedding was hand wriiting (designed by people), may be from people know more about the language, or others whose have some knowledge about textual data and can give some scores for these words based on some topics, or based on the occurrence of the words together.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td><img src=\"images/word_vec4.png\"></td>\n",
    "    <td><img src=\"images/word_vec5.png\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "But **This is Machine learning world** What about deep learning ?\n",
    "\n",
    "- DL = Learn them\n",
    "- Just set how many Hyperparameter you need (embedding, latent factors, features any of these names as you like).\n",
    "- Let the network learn these values (embedding, latent factors, features)\n",
    "\n",
    "## Semantic Queries and analogies\n",
    "\n",
    "**So let's go deeper with some word2vec models we have to see some of its pretty result.**\n",
    "\n",
    "Can you imagine that you have some of information, or words related to some topic or names of some ones you know, but you do not remember the specific name of that person or place, but you can give some ituation about, and the **Word2Vec** can give you a very approximation of what you search for your semantic query that you give the ituation about or about the analogies questions as we can see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "No one need to mention any thing to word2vec, like these words are of places (actually cities), or whose is nearest to some player from egypt to other from Europe.\n",
    "\n",
    "So word2vec can learn that and much more than your intuition, just give it large enough data that mention all the textual or relative information you need.\n",
    "\n",
    "## Word Embeddings Look-up\n",
    "\n",
    "We have moved from the world of machine learning to deep learning to learn about these embedding as we said, just give the model how many of these embedding you need and let the model learn them. \n",
    "\n",
    "So how it can learn using the indices we have give to words, to build the Embedding Matrix (Look-up Table), which have word indices in rows and for each word (row), set how many embeddings you need (between 50 - 300 by practice).\n",
    "\n",
    "<img src=\"images/word_vec6.png\">\n",
    "\n",
    "**So each index will cause one row to be selected and learned**\n",
    "\n",
    "\n",
    "## But Who sets all these weights in the table !! ?\n",
    "\n",
    "With BOW model we was have some of text(reviews, tweets or sentence), and we have the label associated with these sentence either its negative or positive or some thing that we can compare the prediction with and test the loss then applying step by update the wieghts.\n",
    "\n",
    "But most of the data unlabeled, there is no output for each tweets, its just huge data without labeling. And this take us to the world of **unsupervised learning**, which **Word2Vec model** depends on.\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "We have two types of word2vec one of them to predict surrounding words from the center word which is called **Skip-Gram** and other one to predict the center(target word) from the surrounding word and this is called **Continuous Bag of words**\n",
    "\n",
    "# Note !!\n",
    "\n",
    "If we give it a time to read text above, we can notice that **predict !** and **from !**, the output and input, are actually from the data itself, no label is need, the model can learn from the data itself.\n",
    "\n",
    "Actually we not interest in these prediction itself, we interested about the embedding matrix (Look-Up Table) that the model build and learn from the data, which help us to learn the meaning, semantic and synonyms of words, how related words are close to each other in some spaces and other as we saw above from exampels.\n",
    "May be now you can imagine how this capability of word2vec can help in apps like [chatbot, search engine, question answering and other tasks]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Approach\n",
    "\n",
    "In skip-gram approach we trying to predict the surrounding window of words from the center word, as we can see in image, the preidcited word words was **[claude, monet, the and grand]** while the input is just the word **painted** !\n",
    "\n",
    "<img src=\"images/skip_1.png\">\n",
    "\n",
    "Actually skip-gram are n-grams that contain gaps because we skip some tokens while we predict others.\n",
    "\n",
    "But how we feed these inputs to the network, if we skip to next window, and take the word monet as input, then the output is one word before because there is no other words before and 2 words after. !\n",
    "\n",
    "\n",
    "\n",
    "Its like simple neural network of just two layers, **hidden layer** which is build our main **Embedding Matrix**, and consisit of **n nurons as columns**, the **flatten factors(features)** that you need to represent words in, and the **M** rows which the number of **unique words** the vocabulary of your data, as well as same vector dimension of the **one-hot input vector**.\n",
    "\n",
    "The other layer is the output layer, which also consist of M neurons equal to number of unique word in our data, and we get the word that represent the max probability like in image above.\n",
    "\n",
    "**So for example if we have like this sentence:**\n",
    "\n",
    "**sentence = \"Claude Monet painted the Grand Canal of Venice in 1806.\"**\n",
    "\n",
    "We can build our target matrix like that:\n",
    "\n",
    "<img src=\"images/skip_3.png\">\n",
    "\n",
    "\n",
    "## Note !\n",
    "\n",
    "So we can notice that for word **Claude** there is no previous words, but there are two predicted words after it, and as we moved with our window of 5-words we build the table above.\n",
    "\n",
    "Each input is **one-hot-vector**, with 1 in the index of the word and 0 for all other places in the vector.\n",
    "\n",
    "Each word have the chance to be the predicted word for that input, sums all to one, and get the word with the heighest probabilty, then map to 1 to compare with the target table and so on.\n",
    "\n",
    "# What is Next !\n",
    "\n",
    "As we can see above the trained neural network of Continuous Bag of words of our model have its own Embedding table, which each word inside represent the semantic meaning of the word.\n",
    "\n",
    "Thanks to one hot vector and the idea of using index for tokens, as now each row in the weighted (Embedding matrix) represent each word from our unique vocabulary from the data we have. And after training semantically similar words will have similar vectors, **because they were trained to predict similar surrounding words**, and actually This is Purely Magical.\n",
    "\n",
    "## Note !!\n",
    "\n",
    "We should start training from begining each time we have new words because the output and input will have differernt number of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag-of-words Approach\n",
    "\n",
    "While skip-gram predict the surrounding words from the center words, the CBOW is the opposite and predict the center word from the neighbours. So instead of input the center word as one-hot vector, now we will pass the surrounding words as **multi-hot-vector** of ones (1) in index related to these words and 0 in all other places.\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td><img src=\"images/cbow_1.png\"></td>\n",
    "    <td><img src=\"images/cbow_2.png\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "**And if we look at how we feed the data into the network, it will be something like in the image below for same sentence we introduced in skip-gram and let's predict the word [painted]**\n",
    "\n",
    "<img src=\"images/cbow_3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tricks of Word2Vec\n",
    "\n",
    "## Frequent bigrams\n",
    "\n",
    "**New York** as we can see some of words comes together a lot in the context, so we can combine these terms into one term separate them by _ like **New_York** or any symbol and let the network learn also from these n-grams, and the reason of that, these combination of words has different meaning than its individual words.\n",
    "\n",
    "## Subsampling frequent tokens\n",
    "\n",
    "As well as stop words comes a lot in any tweet or document, while they have some sharing meaning in the text, but not very important meaning, so giving these words less weights will help improve the word2vec model.\n",
    "\n",
    "## Negative Sampling\n",
    "\n",
    "To speed up the training of word2vec model, negative sampling comes to light, instead of input one word and predict the surrounding words or the opposite, negative sampling feed two words together and check its validity of these two words comes together within the window then predict 1 other wise predict 0.\n",
    "\n",
    "\n",
    "## Work in our private repo github\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Crawling-twitter-Data-using-twint_v2/blob/main/train_word2vec_from_scratch.py\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Crawling-twitter-Data-using-twint_v2/blob/main/one_gram_model.ipynb\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Crawling-twitter-Data-using-twint_v2/blob/main/full_gram_from_2_to_3.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
